{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#tag:other","title":"Other","text":"<ul> <li>            ZFS Send/Receive          </li> <li>            Zfshost          </li> </ul>"},{"location":"Other/zfs_send_receive/","title":"ZFS Send/Receive","text":"<p><code>zfs send</code> from main server and <code>zfs receive</code> on backup server</p>","tags":["Other"]},{"location":"Other/zfs_send_receive/#general-facts-for-consideration","title":"General Facts For Consideration","text":"<ol> <li>Both <code>zfs send</code> and <code>zfs receive</code> operations require plenty of permissions, which can be assigned to an unprivileged user via <code>zfs allow</code> (we should not run the script as root on either end as most guides suggest)</li> <li>If the source dataset is auto-mounted, it will be attempted to be mounted on the receiving end, which can be a problem</li> <li>If a backup dataset is mounted, there is a possibility (highly likely if <code>atime</code> is on) that it will be modified on the receiving end, semi-breaking future incremental updates</li> <li>For scripted operations, one would need to have an ssh key without passphrase on either the main or the backup server, for the other one.</li> <li>Unlike rsync, which compares source and destination data for diff calculation, <code>zfs send</code> only compares 2 snapshots on the source end to calculate diff, meaning the source and destination must contain a common snapshot, which should be used as the base of the diff on the sending end</li> <li>All properties of the source dataset will be set on the backup dataset as is, which means any property that is <code>inherited</code> on the source will also be set as <code>inherited</code> on the backup, so the actual value may differ (such as the mountpoint)</li> <li>If the machine with the ssh key gets compromised, the other machine is also compromised due to no ssh key passphrase</li> <li>If the source machine gets compromised (ransomwared), the changes can propagate to backups</li> <li>If <code>zfs allow</code> has the <code>destroy</code> permission, a compromised machine can potentially destroy all zfs data including the pools, datasets and the snapshots.</li> <li>Initial <code>send</code> requires that the destination does not have the existing dataset (you can force but I'd rather keep it clean and simple)</li> <li>Incremental <code>send</code> requires the destination state matches the base snapshot used in <code>send</code></li> </ol>","tags":["Other"]},{"location":"Other/zfs_send_receive/#plan","title":"Plan","text":"<ul> <li>Due to <code>1.</code> and <code>4.</code> above, we will use a dedicated unprivileged linux user <code>zfsbackup</code> (not in sudoers) on both the sending and the receiving ends<ul> <li>On the sending end, the user will need the <code>zfs allow</code> perms <code>create,mount,rename,send,snapshot</code> (<code>create</code> and <code>mount</code> are there because they are required by others) (we do not assign the <code>destroy</code> perm due to <code>9.</code>)</li> <li>On the receiving end, the user will need the <code>zfs allow</code> perms <code>atime,canmount,create,mount,readonly,receive,rename</code> (<code>create</code> and <code>mount</code> are there because they are required by others) (we do not assign the <code>destroy</code> perm due to <code>9.</code>)</li> <li>Allow perms are per user per dataset and are inherited. Existing perms can be checked via <code>zfs allow &lt;pool&gt;/&lt;dataset&gt;</code></li> </ul> </li> <li>To prevent issues due to <code>2.</code> and <code>3.</code>, we will use the options <code>-o canmount=noauto -u -o readonly=on</code> because the first 2 will prevent mounting on the receiving end and the 3rd will prevent modifications (for good measure).<ul> <li>We can also add <code>-o atime=off</code> if we do want to mount them, but if it's read only, that should not be necessary.</li> <li>These options are overridden permanently on the receiving end. If we ever have to recover from these backups, we need to kee in mind that our backup datasets have altered properties, which may need to be reverted after a restore. It is prudent to keep these alterations to a minimum so a potential restore doesn't get too complicated.</li> </ul> </li> <li>Picking the receiving end as the initator and holder of the ssh key with no passphrase because that machine is a dedicated backup machine, has no public facing services and thus less likely to be compromised.</li> <li>On the receiving end, we create a new encrypted dataset dedicated to holding backups <code>pool/remotebackups</code> and set the options <code>readonly:on</code> and <code>canmount=noauto</code>. Keep in mind that while the <code>readonly</code> setting is inherited, the <code>canmount</code> setting is not.</li> <li>Because we have multiple pools on the source machine (one for the OS on NVMEs and one for the data/media on HDDs), we'll replicate the same structure on the receiving end to make things simpler:<ul> <li><code>zarray/Books</code> on the source will be backed up to <code>pool/remotebackups/zarray/Books</code> on the receiving end</li> <li><code>zroot/ROOT/debian</code> on the source will be backed up to <code>pool/remotebackups/zroot/ROOT/debian</code> on the receiving end</li> </ul> </li> <li>Since we have to specifically define the source snapshot on the sending end during incremental, and that snapshot state has to match the current state of the backup dataset, we will hardcode snapshot names and make assumptions based on that<ul> <li>We could label the snapshot with date stamps that would match on the sending and the receiving ends, but that would require parsing zfs list outputs and/or keeping a file or db with previous run info. That complicates things and is error prone (hardcoding the snapshot names makes it safer for a cron script to delete them as we reduce the risk of accidentally deleting the wrong snapshot or worse, a dataset)</li> <li>It's much simpler to rely on the following format for snapshot names:<ul> <li><code>remotebackup-snapshot</code> for the snapshot that will be sent and received</li> <li><code>remotebackup-snapshot-1</code> for the snapshot that will be used as the diff base</li> </ul> </li> <li>Prior to send, we rename the snapshots <code>remotebackup-snapshot</code> as <code>remotebackup-snapshot-1</code> on both ends, create a new snapshot <code>remotebackup-snapshot</code> on the sending end, and send that with the diff base <code>remotebackup-snapshot-1</code></li> <li>At the end of the send operation, both ends will have the matching snapshots <code>remotebackup-snapshot</code> and <code>remotebackup-snapshot-1</code></li> <li>Nightly, we have to delete the <code>remotebackup-snapshot-1</code> snapshots on both ends but that has to be done by a privileged user like root cron, otherwise the rename will fail due to existing snapshot with that name</li> <li>At the start of each send operation, we will have only the <code>remotebackup-snapshot</code> on both ends</li> <li>If the structure and naming of the snapshots gets messed up due to errors when running the script, we'll have the script notify us so we can take manual action</li> </ul> </li> </ul>","tags":["Other"]},{"location":"Other/zfs_send_receive/#specific-steps","title":"Specific Steps","text":"","tags":["Other"]},{"location":"Other/zfs_send_receive/#initial-sync","title":"Initial sync","text":"<ol> <li>Let's assume we're syncing <code>zarray/Books</code> on main machine to <code>pool/remotebackups/zarray/Books</code> on the backup machine</li> <li>Add permissions to source dataset on main machine: <code>sudo zfs allow zfsbackup create,mount,rename,send,snapshot zarray/Books</code></li> <li>Create the parent pool/dataset on the backup machine: <code>sudo zfs create -o canmount=noauto -o readonly=on pool/remotebackups/zarray</code></li> <li>Add permissions to backup parent pool/dataset on backup machine: <code>sudo zfs allow zfsbackup atime,canmount,create,mount,readonly,receive,rename pool/remotebackups/zarray</code></li> <li>Log into <code>zfsbackup</code> on the backup machine: <code>su zfsbackup</code></li> <li>Create snapshot on the source dataset via ssh: <code>ssh zfsbackup@MAINSERVERIP zfs snapshot zarray/Books@remotebackup-snapshot</code></li> <li>Perform initial sync via ssh: <code>ssh zfsbackup@MAINSERVERIP zfs send -v zarray/Books@remotebackup-snapshot | zfs receive -o canmount=noauto -u -o readonly=true pool/remotebackups/zarray/Books</code></li> </ol>","tags":["Other"]},{"location":"Other/zfs_send_receive/#subsequent-incremental-sync","title":"Subsequent Incremental Sync","text":"<ol> <li>Log into <code>zfsbackup</code> on the backup machine: <code>su zfsbackup</code></li> <li>Rotate snapshot on the source dataset via ssh: <code>ssh zfsbackup@MAINSERVERIP zfs rename zarray/Books@remotebackup-snapshot zarray/Books@remotebackup-snapshot-1</code></li> <li>Create new snapshot on the source dataset via ssh: <code>ssh zfsbackup@MAINSERVERIP zfs snapshot zarray/Books@remotebackup-snapshot</code></li> <li>Rotate snapshot on the backup dataset: <code>zfs rename pool/remotebackups/zarray/Books@remotebackup-snapshot pool/remotebackups/zarray/Books@remotebackup-snapshot-1</code></li> <li>Perform incremental sync via ssh: <code>ssh zfsbackup@MAINSERVERIP zfs send -vi zarray/Books@remotebackup-snapshot-1 zarray/Books@remotebackup-snapshot | zfs receive -o canmount=noauto -u -o readonly=true pool/remotebackups/zarray/Books</code></li> </ol>","tags":["Other"]},{"location":"Other/zfs_send_receive/#clean-up-after-sync","title":"Clean Up After Sync","text":"<ol> <li>On the main machine edit the root crontab via <code>sudo crontab -e</code> and add an entry to delete the rotated snapshot: <code>17 0 * * * zfs destroy zarray/Books@remotebackup-snapshot-1</code></li> <li>On the backup machine edit the root crontab via <code>sudo crontab -e</code> and add an entry to delete the rotated snapshot: <code>17 0 * * * zfs destroy pool/remotebackups/zarray/Books@remotebackup-snapshot-1</code></li> </ol>","tags":["Other"]},{"location":"Other/zfs_send_receive/#my-ugly-script","title":"My Ugly Script","text":"<pre><code>#!/bin/bash\n\n# Edit the below values\n\n# Set Source Datasets\nSOURCEDATASETS='zarray/Books zarray/Misc zroot/home zroot/ROOT/debian'\n# Set Local Backup Base\nBACKUPBASE='pool/remotebackups'\n# Set remote server\nREMOTESERVER='zfsbackup@&lt;SERVERIP&gt;'\n\n# notify via self hosted server or if that fails, via the public instance\nfn_notify () {\n  curl -Lfs -H \"Title: $1\" -H \"Authorization: Bearer &lt;my token&gt;\" -d \"$2\"  https://ntfy.&lt;mydomain&gt;/zfshost || \\\n    curl -Ls -H \"Title: $1\" -d \"$2\" https://ntfy.sh/&lt;my custom topic&gt;\n}\n\n# Do not make edits below this line\n\necho \"**** Starting sync on $(date)\"\n# Check to make sure remote server is up and connectable\nif ssh -o ConnectTimeout=10 \"${REMOTESERVER}\" true; then\n  echo \"[Success] Remote server is up and connectable\"\nelse\n  echo \"[Failed] Remote server does not seem to be reachable. Aborting zfs sync.\"\n  fn_notify \"Zima Zfs Sync Failed\" \"Remote server does not seem to be reachable. Aborting zfs sync.\"\n  exit 1\nfi\n\nfor SRC in ${SOURCEDATASETS}; do\n  echo \"**********************************\"\n  echo \"**********************************\"\n  echo \"**** Processing dataset ${SRC}\"\n  # check to make sure the latest snapshot exists on local\n  if zfs list -t snapshot \"${BACKUPBASE}/${SRC}\" | grep -q \"@remotebackup-snapshot \"; then\n    echo \"[Success] ${BACKUPBASE}/${SRC}@remotebackup-snapshot exists on local as expected\"\n  else\n    echo \"[Failed] ${BACKUPBASE}/${SRC}@remotebackup-snapshot does not exist on local. Skipping backup of ${SRC}\"\n    fn_notify \"Zima Zfs Sync Failure\" \"${BACKUPBASE}/${SRC}@remotebackup-snapshot does not exist on local. Skipping backup of ${SRC}\"\n    continue\n  fi\n  # check to make sure the renamed snapshot does not exist on local\n  if ! zfs list -t snapshot \"${BACKUPBASE}/${SRC}\" | grep -q \"@remotebackup-snapshot-1 \"; then\n    echo \"[Success] ${BACKUPBASE}/${SRC}@remotebackup-snapshot-1 does not exist on local as expected\"\n  else\n    echo \"[Failed] ${BACKUPBASE}/${SRC}@remotebackup-snapshot-1 already exists on local. Skipping backup of ${SRC}\"\n    fn_notify \"Zima Zfs Sync Failure\" \"${BACKUPBASE}/${SRC}@remotebackup-snapshot-1 already exists on local. Skipping backup of ${SRC}\"\n    continue\n  fi\n  # check to make sure the latest snapshot exists on remote\n  if ssh \"${REMOTESERVER}\" \"zfs list -t snapshot ${SRC} | grep -q \\\"@remotebackup-snapshot \\\"\"; then\n    echo \"[Success] ${SRC}@remotebackup-snapshot exists on remote as expected\"\n  else\n    echo \"[Failed] ${SRC}@remotebackup-snapshot does not exist on remote. Skipping backup of ${SRC}\"\n    fn_notify \"Zima Zfs Sync Failure\" \"${SRC}@remotebackup-snapshot does not exist on remote. Skipping backup of ${SRC}\"\n    continue\n  fi\n  # check to make sure the renamed snapshot does not exist\n  if ! ssh \"${REMOTESERVER}\" \"zfs list -t snapshot ${SRC} | grep -q \\\"@remotebackup-snapshot-1 \\\"\"; then\n    echo \"[Success] ${SRC}@remotebackup-snapshot-1 does not exist on remote as expected\"\n  else\n    echo \"[Failed] ${SRC}@remotebackup-snapshot-1 already exists on remote. Skipping backup of ${SRC}\"\n    fn_notify \"Zima Zfs Sync Failure\" \"${SRC}@remotebackup-snapshot-1 already exists on remote. Skipping backup of ${SRC}\"\n    continue\n  fi\n  # Rename snapshots on both local and remote\n  echo \"Renaming snapshot on the local\"\n  zfs rename \"${BACKUPBASE}/${SRC}@remotebackup-snapshot\" \"${BACKUPBASE}/${SRC}@remotebackup-snapshot-1\"\n  if [ \"$?\" -ne 0 ]; then\n    echo \"[Failed] Renaming snapshot on the local failed. Skipping backup of $SRC\"\n    fn_notify \"Zima Zfs Sync Failure\" \"Renaming snapshot on the local failed. Skipping backup of $SRC\"\n    continue\n  fi\n  echo \"Renaming snapshot on the remote\"\n  ssh \"${REMOTESERVER}\" \"zfs rename ${SRC}@remotebackup-snapshot ${SRC}@remotebackup-snapshot-1\"\n  if [ \"$?\" -ne 0 ]; then\n    echo \"[Failed] Renaming snapshot on the remote failed. Skipping backup of $SRC\"\n    fn_notify \"Zima Zfs Sync Failure\" \"Renaming snapshot on the remote failed. Skipping backup of ${SRC}\"\n    continue\n  fi\n  # create snapshot on remote\n  echo \"Creating snapshot on the remote\"\n  ssh \"${REMOTESERVER}\" \"zfs snapshot ${SRC}@remotebackup-snapshot\"\n  if [ \"$?\" -ne 0 ]; then\n    echo \"[Failed] Creating snapshot on the remote failed. Skipping backup of $SRC\"\n    fn_notify \"Zima Zfs Sync Failure\" \"Creating snapshot on the remote failed. Skipping backup of ${SRC}\"\n    continue\n  fi\n  # Initiate zfs send\n  echo \"Initiating zfs send for ${SRC}\"\n  ssh \"${REMOTESERVER}\" \"zfs send -vi ${SRC}@remotebackup-snapshot-1 ${SRC}@remotebackup-snapshot\" | zfs receive -o readonly=on -o canmount=noauto -u \"${BACKUPBASE}/${SRC}\"\n  if [ \"$?\" -ne 0 ]; then\n    echo \"[Failed] Sending snapshot failed. Skipping backup of $SRC\"\n    fn_notify \"Zima Zfs Sync Failure\" \"Sending snapshot failed. Skipping backup of ${SRC}\"\n    continue\n  fi\n  SRC_COMPLETED=\"${SRC} ${SRC_COMPLETED}\"\n  echo \"[Success] Zfs send for ${SRC} completed\"\n  echo \"**********************************\"\n  echo \"**********************************\"\n  echo \"**********************************\"\ndone\nif [ -n \"${SRC_COMPLETED}\" ]; then\n  fn_notify \"Zima Zfs Sync Completed\" \"The following datasets were successfully synced: ${SRC_COMPLETED}\"\nfi\necho \"Ended sync on $(date)\"\n</code></pre> <p>This script is not pretty, has plenty of duplicate commands and statements. I prefer function over form. My primary goals with this script are error detection and verbosity. Because I rely on hardcoded snapshot names, and because zfs send receive needs the snapshots perfectly synced on both sides, I really want to make sure I preserve the snapshots. Otherwise, incremental sync will break and I'll need to do a full sync again. If anything goes wrong, this script will immediately halt, log the error and notify via ntfy so I can go in and fix it manually.</p> <p>I have this script set to run weekly via the <code>zfsbackup</code> user's cron on the backup machine. I also have the root cron scripts running nightly to delete the rotated snapshots (named <code>@remotebackup-snapshot-1</code>) for each source and backup dataset on both machines.</p>","tags":["Other"]},{"location":"Other/zfshost/","title":"Zfshost","text":"<p>Details of my new mini homelab server</p>","tags":["Other"]},{"location":"Other/zfshost/#hardware","title":"Hardware","text":"<ul> <li>Jonsbo N2 case</li> <li>Purple CWWK/Topton board with i3-N305 processor and Jonsbo CPU fan (2 eth instead of 4, pci-e port has its own lane so it can be used alongside both M2 ports)</li> <li>Corsair 48GB mem stick (4800 version)</li> <li>2 1TB WD Blue Nvme on the board M2 slots (1X each unfortunately so not full speed) in a ZFS mirrored pool for the OS</li> <li>5 8TB WD (mostly red) HDD in drive bays in a ZFS RAIDZ2 pool (24TB usable) for long term storage</li> <li>1 256GB SSD (haven't decided what to use it for yet, probably temp files like <code>/var/lib/docker</code> content to increase the life of NVMEs)</li> <li>750W Thermaltake SFX PSU</li> <li>Coral dual tpu in a pci-e adapter</li> <li>PiKVM ATX controller (hooked up to motherboard header pins)</li> <li>USB header splitter and adapter to connect the case's USB A and USB C ports to the motherboard's single USB 3 header port (I'm honestly surprised this system worked because plenty of folks online said it wouldn't)<ul> <li>USB 3 header splitter</li> <li>USB 3 to type C header adapter</li> </ul> </li> <li>6 in 1 SATA cable (case is very cramped and full SATA cables make ot more crowded in there)</li> <li>1 32GB sdcard as UEFI #1 with ZfsBootMenu</li> <li>1 32GB USB flash drive (in mobo USB slot) as UEFI #2 with ZfsBootMenu (for redundancy)</li> <li>APC UPS connected to a USB2 port in the back</li> </ul>","tags":["Other"]},{"location":"Other/zfshost/#software","title":"Software","text":"","tags":["Other"]},{"location":"Other/zfshost/#uefi","title":"UEFI","text":"<p>Custom build of ZfsBootMenu on 2 separate devices (1 sdcard and 1 USB stick for redundancy). Used the container build method but added Tailscale and NTFY.</p> <p>ZfsBootMenu waits for passphrase and once entered, unlocks all zfs pools, searches for bootable datasets, provides them as options (for optional multi-boot). It also allows for zfs recovery and snapshot management. It also caches the ZFS keys when launching the OS so pools/datasets can be mounted automatically.</p> <p>Custom build includes: - Tailscale (for remote access) - Dropbear (for remote access) - Notifications via Discord and NTFY (so that when it boots after a power failure, I'm notified to ssh in and enter the ZFS unlock passphrase to minimize down time)</p> <p>Tip: When setting up your network access in ZfsBootMenu with a static IP, use the interface name <code>eth0</code> or <code>eth1</code> because that's what the VoidLinux based ZfsBootMenu uses instead of the more modern <code>enpXsX</code> scheme. For instance if your Debian install lists two network interfaces <code>enp4s0</code> and <code>enp5s0</code>, ZBM will identify them as <code>eth0</code> and <code>eth1</code> respectively.</p> <p>Here's my tree for the zfsbootmenu build folder: <pre><code>/etc/zfsbootmenu\n\u251c\u2500\u2500 build                               - Output folder for built EFIs. Always keeps one older version named backup by default.\n\u2502   \u251c\u2500\u2500 zfsbootmenu-backup.EFI            I do \"cp -a /etc/zfsbootmenu/build/zfsbootmenu.EFI /boot/efi/EFI/BOOT/BOOTX64.EFI\" to\n\u2502   \u2514\u2500\u2500 zfsbootmenu.EFI                   update the EFI on the mounted sd card (and USB flash drive mounted to \"/boot/efi2\")\n\u251c\u2500\u2500 cleanup.d\n\u2502   \u2514\u2500\u2500 hostfiles                       - Cleans up temp files at the end of build\n\u251c\u2500\u2500 config.yaml                         - Enables initcpio and other settings for the build\n\u251c\u2500\u2500 dropbear                            - Contains the dropbear config and keys\n\u2502   \u251c\u2500\u2500 dropbear.conf                   - Contains the listen port argument\n\u2502   \u251c\u2500\u2500 dropbear_ecdsa_host_key         - Host keys don't need to exist as they would be created on first run, then they would be reused\n\u2502   \u251c\u2500\u2500 dropbear_ecdsa_host_key.pub\n\u2502   \u251c\u2500\u2500 dropbear_ed25519_host_key\n\u2502   \u251c\u2500\u2500 dropbear_ed25519_host_key.pub\n\u2502   \u251c\u2500\u2500 dropbear_rsa_host_key\n\u2502   \u251c\u2500\u2500 dropbear_rsa_host_key.pub\n\u2502   \u2514\u2500\u2500 root_key                        - Contains my public ssh key for key based ssh login\n\u251c\u2500\u2500 initcpio\n\u2502   \u251c\u2500\u2500 hooks                           - Contains runtime hooks for when ZfsBootMenu loads\n\u2502   \u2502   \u251c\u2500\u2500 ntfy                        - Sends notification with a 10 sec delay (also has a discord notification added)\n\u2502   \u2502   \u251c\u2500\u2500 dropbear                    - Starts the dropbear service\n\u2502   \u2502   \u251c\u2500\u2500 rclocal                     - Sets up networking\n\u2502   \u2502   \u2514\u2500\u2500 tailscale                   - Starts the tailscale service\n\u2502   \u251c\u2500\u2500 install                         - Contains build time hooks for building ZfsBootMenu\n\u2502   \u2502   \u251c\u2500\u2500 ntfy                        - Copies the ca-certificates into UEFI for curl ssl and enables the runtime hook\n\u2502   \u2502   \u251c\u2500\u2500 dropbear                    - Creates host keys if needed, copies the keys and dropbear binary into UEFI, and enables the runtime hook\n\u2502   \u2502   \u251c\u2500\u2500 rclocal                     - Enables the runtime hooks that enable networking for ZfsBootMenu\n\u2502   \u2502   \u2514\u2500\u2500 tailscale                   - Copies the tailscale and dep binaries and the state file into UEFI, and enables the runtime hook\n\u2502   \u2514\u2500\u2500 rc.local                        - Contains the networking config (interface, routes and dns)\n\u251c\u2500\u2500 mkinitcpio.conf.d                   - Contains the conf files for initcpio modules (hooks and binaries) needed to be installed/enabled in ZfsBootMenu\n\u2502   \u251c\u2500\u2500 ntfy.conf                       - Adds the curl binary and enables ntfy hooks\n\u2502   \u251c\u2500\u2500 dropbear.conf                   - Enables dropbear hooks\n\u2502   \u251c\u2500\u2500 network.conf                    - Enables the rclocal hooks for networking\n\u2502   \u2514\u2500\u2500 tailscale.conf                  - Enables the tailscale hooks and adds the ip and dhclient binaries\n\u251c\u2500\u2500 rc.d                                - Contains scripts to prepare the build container environment\n\u2502   \u251c\u2500\u2500 dropbear                        - Copies the dropbear config files into the build container filesystem\n\u2502   \u2514\u2500\u2500 tailscale                       - Copies the tailscale config files into the build container filesystem\n\u251c\u2500\u2500 tailscale                           - Contains the tailscale config and state files\n\u2502   \u251c\u2500\u2500 tailscaled.conf\n\u2502   \u2514\u2500\u2500 tailscaled.state\n\u251c\u2500\u2500 zbm-builder.conf                    - Config for the zbm builder\n\u2514\u2500\u2500 zbm-builder.sh                      - Script that builds the ZfsBootMenu UEFI in a docker container and outputs to build folder\n\n10 directories, 31 files\n</code></pre> With all config files in place, simply cd to the folder and run <code>zbm-builder.sh</code> from inside that folder. It creates a docker build container with the current folder mounted, and it will use the various rc.d and mkinitcpio init files to prepare the build environment, build the UEFI image, and output to <code>build/</code>. You can then copy it to your boot drive at <code>/EFI/BOOT/BOOTX64.EFI</code>.</p>","tags":["Other"]},{"location":"Other/zfshost/#os","title":"OS","text":"<p>Debian (Bookworm) server installed on the dual NVME mirrored ZFS pool. Protected by ZFS passphrase (key also stored in the pool for caching and automounting). Install followed ZfsBootMenu instructions: https://docs.zfsbootmenu.org/en/latest/guides/debian/bookworm-uefi.html with a few changes: - Followed the <code>Separate Boot Device</code> instructions but skipped the parts about creating a partition on the NVME for the ZFS pool (because I'm using a separate device for the UEFI). I instead created the ZFS pool using the full disk. - Created an encrypted pool. - Instructions make you use a single disk for the pool. Afterwards I converted it to a dual mirror by adding the second NVME to the pool.</p> <p>Tip: Stock Debian server does not come with any ntp client/service installed so eventually time shifts and operations involving time based cryptography start failing. Don't forget to install the ntp package (or alternative) to keep time synced. I found out through Authelia crashing on start.</p> <p>When I first set up stock Debian, I noticed some startup log errors about the GPU. I also noticed that hw transcode wasn't working in containers when passing the DRI device. Researching the issue, I became aware of two potential causes. Unfortunately, I applied both fixes at once so I can't tell for sure now if both are necessary or if one would suffice. But I personally would have eventually implemented both of these for other reasons anyway: - Upgraded the kernel to the backports version because I thought my gpu was too new for the stock Debian kernel for full support. That may or may not be true, but I needed the newer kernel for other reasons as well so I'm keeping it. - Added <code>non-free-firmware</code> to Debian apt sources (<code>/etc/apt/sources.list</code>) so the Intel drivers/firmware are upgraded.</p> <p>Upgrading the kernel lead to another issue, this time with the Coral pci-e driver for the Edge TPU. Apparently, Google sucks at keeping the driver up to date. Following their official directions results in an error during compilation. After some rabbit hole exploring, I came across an unmerged PR that solves the issue with compiling on the 6.12 kernel: https://github.com/google/gasket-driver/pull/35 I had to locally build the DKMS deb with that patch and install it.</p>","tags":["Other"]},{"location":"Other/zfshost/#zfs","title":"ZFS","text":"","tags":["Other"]},{"location":"Other/zfshost/#zfs-pools","title":"ZFS Pools","text":"<p>OS pool was created above. I then created an encrypted ZFS pool made up of the 5 8TB HDDs in RAIDZ2 (2 parity) for a usable space of 24TB. When creating, make sure to use the devices <code>/dev/disk/by-id</code> instead of <code>/dev/sdX</code>. <code>lsblk</code> is your friend.</p> <p>Resources: - https://docs.zfsbootmenu.org/en/latest/guides/debian/bookworm-uefi.html#create-the-zpool - https://virtualize.link/Other/zfs/#creation</p>","tags":["Other"]},{"location":"Other/zfshost/#zfs-dataset-structure","title":"ZFS Dataset Structure","text":"<p>My home folder is on its own dataset <code>zroot/ROOT/home</code> and it contains all my docker app data (persistent data) at <code>/home/aptalca/appdata</code>.</p> <p><code>/var/lib/docker</code> is on its own dataset <code>zroot/docker</code> along with my openvscode-server's dind docker folder and the <code>modcache</code> as all of that is ephemeral and I can set the least aggressive snapshot plan for it.</p> <p>My long term storage pool (HDD based) <code>zarray</code> has datasets for <code>Media</code>, <code>Books</code>, <code>Pictures</code> and <code>Misc</code> (contains backups and other miscellanous data).</p>","tags":["Other"]},{"location":"Other/zfshost/#zfs-auto-mount","title":"ZFS Auto Mount","text":"<p>Unfortunately Debian's ZFS package no longer auto mounts pools and datasets. Apparently it was disabled due to a bug that affected some people a while back.</p> <p>To enable, follow the instructions here (essentially do <code>sudo systemctl edit zfs-mount</code> and add the missing <code>-l</code> to the <code>zfs mount</code> command so it loads the cached keys when attempting the mount).</p> <p>To make sure the key for the HDD pool is cached by ZfsBootMenu, make sure to add the property <code>org.zfsbootmenu:keysource</code> to the pool. If using the same passphrase as the OS pool (highly recommended), you'd set it to <code>org.zfsbootmenu:keysource=\"zroot/ROOT/debian\"</code> along with <code>keylocation=file:///etc/zfs/zroot.key</code> and <code>keyformat=passphrase</code>. That way ZfsBootMenu will know to unlock and mount the OS pool first, and cache the key in <code>/etc/zfs/zroot.key</code> for both pools.</p>","tags":["Other"]},{"location":"Other/zfshost/#zfs-snapshots","title":"ZFS Snapshots","text":"<p>The auto-snapshot package is highly recommended. By default it does frequent (every 15 minutes), hourly, daily, weekly and monthly snapshots on all pools and datasets. The behavior can be customized by setting properties in pools and datasets.</p> <p>I have disabled daily, weekly and monthly on the OS dataset (<code>zroot/ROOT/debian</code>). I also moved <code>/var/lib/docker</code> out of the OS dataset into a newly created <code>zroot/docker</code> dataset as it's all ephemeral data and did not need to mingle it with the rest of the OS.</p>","tags":["Other"]},{"location":"Other/zfshost/#zfs-alerts","title":"ZFS Alerts","text":"<p>By default the zfs package on Debian enables monthly ZFS scrubs. The alerts are managed by ZED, which can be customized to send notifications via various methods.</p> <p>I enabled NTFY via instructions here and emails via msmtp (no mta package installed, just the <code>/etc/mail.rc</code> edited with the content <code>set sendmail=/usr/bin/msmtp</code> and <code>ZED_EMAIL_PROG=\"mail\"</code> set in <code>zed.rc</code>.</p> <p>I enabled both for redundancy. I use my own self hosted NTFY server and there are brief instances where it may not be accessible. In that case, email notification is a good backup.</p>","tags":["Other"]},{"location":"Other/zfshost/#apc-ups","title":"APC UPS","text":"<p>Set up apcupsd with instructions here.</p> <p>I created a custom scripts folder at <code>/etc/apcupsd/customscripts</code> and copied all the scripts into that folder to prevent the scripts from getting overwritten during a package update.</p> <p>In <code>/etc/apcupsd/apcupsd.conf</code>, I set <code>SCRIPTDIR /etc/apcupsd/customscripts</code>.</p> <p>In <code>/etc/apcupsd/customscripts/apccontrol</code> I set <code>SCRIPTDIR=/etc/apcupsd/customscripts</code></p> <p>And I modified the scripts <code>onbattery</code>, <code>offbattery</code> and <code>doshutdown</code> to add email and NTFY notifications. As an example, my <code>onbattery</code> script contains the following: <pre><code>printf \"To: myemail@gmail.com\\nFrom: Zfshost &lt;myemail@gmail.com&gt;\\nSubject: Power loss at home\\n\\nPower went out at home on %s\\n\\n%s\" \"$(/usr/bin/date)\" \"$(/sbin/apcaccess status)\" | /usr/bin/msmtp --auth=on --tls=on --host smtp.gmail.com --port 587 --user myemail@gmail.com --read-envelope-from --read-recipients --password 'echo &lt;gmail app password&gt;' --logfile /home/aptalca/msmtp.log\nchown 1000:1000 /home/aptalca/msmtp.log\ncurl -fs \\\n  -H \"Authorization: Bearer &lt;my_token&gt;\" \\\n  -d \"Power went out at home on $(date)\" \\\n  https://ntfy.&lt;mydomain&gt;/zfshost || \\\ncurl \\\n  -d \"Power went out at home on $(date)\" \\\n  https://ntfy.sh/&lt;mycustomtopic&gt;\nexit 0\n</code></pre> So it first sends me an email, then sends me an NTFY notification through my self hosted instance. If that fails, it notifies me through the public instance of NTFY (which is less reliable as its notifications aren't totally instant, but it has better uptime than my self hosted instance, therefore it's a decent backup).</p> <p>I get notified when the power goes out, when the power comes back (if within the DELAY period set) and when the server decides to shut down.</p>","tags":["Other"]},{"location":"Other/zfshost/#smart-alerts","title":"SMART Alerts","text":"<p>Installed the smartmontools package and modified the <code>/etc/smartmontools/smartd.conf</code> to enable both email alerts (via msmtp) and via NTFY.</p> <p>It performs automatic tests and monitors disk tempreature. Notifies if there are any issues.</p>","tags":["Other"]},{"location":"Other/zfshost/#docker","title":"Docker","text":"<p>Installed docker from the official repos: https://docs.docker.com/engine/install/debian/</p> <p>Put the <code>/var/lib/docker</code> contents into the ZFS dataset <code>zroot/docker</code> mounted to <code>/mnt/docker</code> by adding <code>\"data-root\": \"/mnt/docker/docker\"</code> into <code>/etc/docker/daemon.json</code> so I could disable the auto snapshots. Currently debating whether to move that to the SSD instead (the one connected to the 6th SATA port on the mobo and not added to any ZFS pools).</p> <p>I also put the modmanager data (<code>/mnt/docker/modmanager</code>) as well as the <code>/var/lib/docker</code> content of my <code>openvscode-server</code>'s DIND (<code>/mnt/docker/dind-openvscode-server</code>) in that same dataset. None of those 3 sources of data need to be retained and can easily be recreated by running the containers again.</p> <p>The persistent data of all containers reside under <code>/home/aptalca/appdata</code>. The home folder is on its own ZFS dataset with an aggressive snapshot schedule as that data is crucial. Compose yamls (one for Immich specifically and one for all others) are at <code>/home/aptalca</code>.</p> <p>Keeping most container arguments in a single yaml results in a very long file. I took advantage of yaml anchors described here to make it more manageable.</p>","tags":["Other"]},{"location":"Other/zfshost/#docker-networking-and-macvlan","title":"Docker Networking and Macvlan","text":"<p>We generally recommend against using macvlan for docker containers however there are a few use cases where macvlans are necessary. One use case is when we want to manage container traffic via source IP based firewall rules on the router (Opnsense). I have a few containers that I do that for, such as bypassing VPN to upload backups to Backblaze B2, or putting certain containers on a different VPN that supports port forwarding (for . . . reasons).</p> <p>One major drawback of macvlan is that devices or containers on a macvlan can't access or be accessed by the host that is not on the macvlan. That is a kernel restriction not specific to docker, but an annoying issue. To get around it, you can create a virtual interface and route the connections through there as the kernel restriction affects the main interface only.</p> <p>What I did is, create the following custom interface config and place it in <code>/etc/network/interfaces.d/</code>, which not only sets up my server ethernet with a static IP, but it also creates a virtual interface along with a route for it that has a lower metric (higher priority) than the main interface so all lan connections go over the virtual interface. That way, macvlan containers can connect the host and vice versa.</p> <pre><code>    auto lan1\n    iface lan1 inet static\n        address 192.168.1.50/24\n        gateway 192.168.1.1\n\n    auto lan1.10\n    iface lan1.10 inet manual\n\n    post-up ip link add vhost0 link lan1 type macvlan mode bridge\n    post-up ip link set vhost0 up\n    post-up ip route add 192.168.1.0/24 dev lan1 proto kernel scope link src 192.168.1.50 metric 1\n    post-up ip route del 192.168.1.0/24 dev lan1 proto kernel scope link src 192.168.1.50\n    post-up ip route add 192.168.1.0/24 dev vhost0 proto kernel scope link src 192.168.1.50\n    post-up ip link add vhost0.10 link lan1.10 type macvlan mode bridge\n    post-up ip link set vhost0.10 up\n</code></pre> <p><code>lan1</code> is my main ethernet interface and <code>lan1.10</code> is the <code>vlan 10</code> tagged version. <code>vhost</code> is the virtual interface and <code>vhost0.10</code> is the <code>vlan 10</code> tagged version. The post up arguments delete the default lan route for my <code>lan1</code> interface and replace it with one that has <code>metric 1</code>, so the <code>vhost</code> route for the lan is preferred.</p> <p>You'll notice that I'm using <code>lan1</code> for my main physical interface instead of the detected <code>enp4s0</code>. That's because the auto generated name is based on the pci bus and slot ids and unfortunately not static. When I plug and unplug my coral into the pci-e slot, the interface name flip flops between <code>enp3s0</code> and <code>enp4s0</code>, which is far from ideal when linking virtual interfaces, using static IP rules, and customizing routes.</p> <p>So I had to resort to creating a permanent name for my main physical interface via a udev rule that matches the name to the interface's MAC address. I created a file at <code>/etc/udev/rules.d/10-custom-lan1.rules</code> with the following content: <pre><code>SUBSYSTEM==\"net\", ACTION==\"add\", DRIVERS==\"?*\",\nATTR{address}==\"&lt;my MAC address&gt;\", NAME=\"lan1\"\n</code></pre> So now my main physical interface is listed as <code>lan1</code>, with the non-permanent name listed as <code>altname enp4s0</code>.</p> <p>When I create a macvlan network in docker, I use the parent <code>vhost0</code> or <code>vhost0.10</code> as appropriate.</p> <pre><code>networks:\n  macvlan:\n    name: macvlan\n    driver: macvlan\n    driver_opts:\n      parent: vhost0\n    ipam:\n      driver: default\n      config:\n       - subnet: 192.168.1.0/24\n         gateway: 192.168.1.1\n  macvlan.10:\n    name: macvlan.10\n    driver: macvlan\n    driver_opts:\n      parent: vhost0.10\n    ipam:\n      driver: default\n      config:\n       - subnet: 192.168.2.0/24\n         gateway: 192.168.2.1\n</code></pre>","tags":["Other"]},{"location":"Other/zfshost/#immich-stack","title":"Immich Stack","text":"<p>Immich set up involves a stack of 4 containers. I elected to keep it in a separate yaml because updates require comparing the yaml contents to the upstream's recommended yaml (that gets updated with each upstream release) and carefully editing/updating pinned image tags and other arguments.</p> <p>Disclaimer: I'm pretty anal retentive about my family photos. I need to keep original copies of photos backed up both locally and remotely and can't stand the idea of losing photos. Perhaps my descendants (if any are into history or are sentimental) will thank me for it, who knows. . .</p> <p>Prior to setting up Immich, I would manually transfer photos from the family cell phones to my server via smb. That way I kept a master record of all family photos that would get backed up to Backblaze B2 weekly. But as with any manual operation, the frequency of transfers was not great and I risked losing data between transfers. In fact, I noticed that some photos got corrupted on the device some days or weeks after they were taken (I know they werent't initially corrupt because they were uploaded to Google Photos properly shortly after they were taken). Somehow between the photos being taken (and subsequently uploaded Google) and my manual transfer, something accessed the photos on the device and corrupted them. No idea what, but I wanted to prevent that in the future.</p> <p>When I set up Immich (as a backup to Google Photos because there is always the possibility of Google making a change that would make it unfeasible for me to continue using it), I wanted to make sure Immich only had read only access to my photos (since it's still considered alpha software and has breaking changes). I added my photos library to Immich as an external library and mounted the path in the Immich container as read-only. For uploads, I set up Nextcloud, which the phones would upload to Nextcloud automatically, and a nightly cron script would copy the photos to my photos library, which Immich would scan and detect shortly after.</p> <p>Unfortunately Nextcloud's photo sync function turned out to be unreliable. First I noticed it was stripping geotags from photos when uploading because it needed a special Android permission. After assigning that permission, it worked for a couple of months but then broke again. Apparently Google blocked Nextcloud from accessing that permission. That became a deal breaker for me and I had to migrate. It also had other bugs like trying to upload temp files (hidden, starting with a period in their name) and sometimes succeeding but often failing with errors (I don't know which is worse), even though the option in settings to ignore hidden files was checked. The bug report pending 5 months with no team response (other than adding a couple of tags) and reddit posts about the issue going back much longer.</p> <p>Hesitantly, I switched to using Immich's Android app to upload photos, which meant that Immich would have write access to all my photos going forward. The compromise I came up with is letting Immich upload to its internal library, then a nightly cron script copies the newly added photos to a separate folder, which I use for long term storage and back up to Backblaze from. So if Immich does something unexpected or unwanted to the photos in the future, I would still have untouched copies in that other folder. Sure, if Immich messes with the photos right after upload and before the nightly cron copy, I'm out of luck, but it's a fairly small risk. I'm more worried about a potential Immich bug that would mass modify older photos, in which case I would be protected.</p> <p>To sum up, I have the following folders on the server: - <code>/mnt/user/Pictures/All</code>: contains all original photos taken priot to starting with Immich. Backed up weekly to Backblaze B2 - <code>/mnt/user/Pictures/All-Immich</code>: contains all original photos uploaded by Immich and copied here nightly. Backed up weekly to Backblaze B2 - <code>/mnt/user/Immich</code>: Immich's internal library. All mobile uploaded photos go here.</p> <p>As you can see there is duplication between the last two paths as the nightly cron script copies photos from one folder to the other. To prevent using double the storage space, I could use hardlinks, but that would mean Immich making changes to one copy would also modify the other copy and that would defeat the purpose. I'm currently looking into ZFS deduplication feature. In the past it was highly recommended against due to really high resource utilization. But ZFS 2.3 apparently introduced a <code>fast dedup</code> feature (rather a collection of features and improvements) that reduces the resource utilization and makes the feature more useful. ZFS dedup works at a block level and would prevent double storage fo copied files in different folders. It would also simply duplicate the blocks if Immich ever made a change to its own copy so the copied version remains untouched. I'll have to do some tests before jumping onto it. Debian backports just got support for ZFS 2.3.0 (as of 2025-03-20) so it's brand new.</p>","tags":["Other"]},{"location":"Other/zfshost/#vms","title":"VMs","text":"<p>I was quite spoiled by Unraid's VM management interface, which was pretty good. I mainly used two VMs: - Win11 VM for a dual purpose:   - Adobe Suite (I don't want to install any Adobe stuff on my main computer because Creative Cloud spreads like a virus with its million background processes that are impossible to disable as it turns into a game of whack-a-mole)   - Bypassing VPN (my entire internet connection goes over Torguard via Wireguard tunnels for privacy but some websites block me. In those crucial cases, I use the browser of the VM, which bypasses the VPN via firewall rules, which is much easier than modifying firewall rules for my main computer whenever I need to access such a website). - Hackintosh VM specifically for tracking Airtags that I put in various bags. Apparently you can only track Airtags via an Apple device and not on the web (through iCloud). Thankfully a hackintosh VM counts as an Apple device.</p> <p>I was really dreading migrating them and using cli kvm tools to manage them. But I found out about Cockpit which made it pretty easy to move the VMs over and manage them. Its gui even has a built-in VNC viewer. For the Hackintosh VM I had to manually edit the xml for a couple of things but most other things I could do from the gui. It was as simple as moving the VM disk files over to the new server and importing them while selecting the necessary info like cpu and network devices.</p> <p>When I need to fire up one of the VMs I just do it from the Cockpit interface. Bonus: It also has a pretty good ZFS module for viewing various stats about storage and snapshots and all that.</p>","tags":["Other"]}]}